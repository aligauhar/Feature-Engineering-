{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc9277-a72e-4304-b8c4-8a0d59d29b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ace64964-b8ab-439f-87de-2b1ec4f40817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of your dataset:  book1.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  Value 1  Value 2  Value 3  Value 4       time\n",
      "0  Feature 1 is         1        2        3        4      21345\n",
      "1  Feature 2 am         2        3        4        5  324234243\n",
      "2      Feature 3        3        4        5        6  324324243\n",
      "3      Feature 4        4        5        6        7  234243243\n",
      "Your dataset types:\n",
      "1. NLP\n",
      "2. Vision\n",
      "3. Time Series\n",
      "4. Other\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the dataset type:  1\n",
      "Do you want to exclude any features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features in the dataset:\n",
      "Index(['Feature', 'Value 1', 'Value 2', 'Value 3', 'Value 4', 'time'], dtype='object')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a column to be excluded or 'n' to stop:  time\n",
      "Enter a column to be excluded or 'n' to stop:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after excluding features:\n",
      "         Feature  Value 1  Value 2  Value 3  Value 4\n",
      "0  Feature 1 is         1        2        3        4\n",
      "1  Feature 2 am         2        3        4        5\n",
      "2      Feature 3        3        4        5        6\n",
      "3      Feature 4        4        5        6        7\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to add new fields to the dataset? (y/n):  n\n",
      "Do you want to handle missing data? (y/n):  n\n",
      "Do you want to handle categorical data? (y/n):  n\n",
      "Do you want to scale and normalize the data? (y/n):  n\n",
      "Do you want to handle outliers? (y/n):  n\n",
      "Do you want to handle skewed distributions? (y/n):  n\n",
      "Do you want to concatenate any columns for NLP processing? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns for concatenation:\n",
      "Index(['Feature', 'Value 1', 'Value 2', 'Value 3', 'Value 4'], dtype='object')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the names of columns to concatenate (comma-separated):  Feature,Value 1\n",
      "Enter the name for the new concatenated column:  col\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after concatenation:\n",
      "         Feature  Value 1  Value 2  Value 3  Value 4              col\n",
      "0  Feature 1 is         1        2        3        4  Feature 1 is  1\n",
      "1  Feature 2 am         2        3        4        5  Feature 2 am  2\n",
      "2      Feature 3        3        4        5        6      Feature 3 3\n",
      "3      Feature 4        4        5        6        7      Feature 4 4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the column containing NLP text data (you can use your concatenated column also):  col\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Text:\n",
      "                tokens\n",
      "0  [feature, 1, is, 1]\n",
      "1  [feature, 2, am, 2]\n",
      "2      [feature, 3, 3]\n",
      "3      [feature, 4, 4]\n",
      "\n",
      "Text after stop words removal:\n",
      "            tokens\n",
      "0  [feature, 1, 1]\n",
      "1  [feature, 2, 2]\n",
      "2  [feature, 3, 3]\n",
      "3  [feature, 4, 4]\n",
      "\n",
      "DataFrame with Tokenized Text:\n",
      "         Feature  Value 1  Value 2  Value 3  Value 4              col   \n",
      "0  Feature 1 is         1        2        3        4  Feature 1 is  1  \\\n",
      "1  Feature 2 am         2        3        4        5  Feature 2 am  2   \n",
      "2      Feature 3        3        4        5        6      Feature 3 3   \n",
      "3      Feature 4        4        5        6        7      Feature 4 4   \n",
      "\n",
      "            tokens stopwords_removed_text  \n",
      "0  [feature, 1, 1]            feature 1 1  \n",
      "1  [feature, 2, 2]            feature 2 2  \n",
      "2  [feature, 3, 3]            feature 3 3  \n",
      "3  [feature, 4, 4]            feature 4 4  \n",
      "\n",
      "TF-IDF DataFrame:\n",
      "         am   feature        is\n",
      "0  0.000000  0.462637  0.886548\n",
      "1  0.886548  0.462637  0.000000\n",
      "2  0.000000  1.000000  0.000000\n",
      "3  0.000000  1.000000  0.000000\n",
      "\n",
      "NLP features engineered:\n",
      "         Feature  Value 1  Value 2  Value 3  Value 4              col   \n",
      "0  Feature 1 is         1        2        3        4  Feature 1 is  1  \\\n",
      "1  Feature 2 am         2        3        4        5  Feature 2 am  2   \n",
      "2      Feature 3        3        4        5        6      Feature 3 3   \n",
      "3      Feature 4        4        5        6        7      Feature 4 4   \n",
      "\n",
      "            tokens stopwords_removed_text  \n",
      "0  [feature, 1, 1]            feature 1 1  \n",
      "1  [feature, 2, 2]            feature 2 2  \n",
      "2  [feature, 3, 3]            feature 3 3  \n",
      "3  [feature, 4, 4]            feature 4 4  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to remove highly correlated features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features removed: {'Value 4', 'Value 2', 'Value 3'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to apply dimensionality reduction? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset:\n",
      "         Feature  Value 1              col           tokens   \n",
      "0  Feature 1 is         1  Feature 1 is  1  [feature, 1, 1]  \\\n",
      "1  Feature 2 am         2  Feature 2 am  2  [feature, 2, 2]   \n",
      "2      Feature 3        3      Feature 3 3  [feature, 3, 3]   \n",
      "3      Feature 4        4      Feature 4 4  [feature, 4, 4]   \n",
      "\n",
      "  stopwords_removed_text  \n",
      "0            feature 1 1  \n",
      "1            feature 2 2  \n",
      "2            feature 3 3  \n",
      "3            feature 4 4  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the feature-engineered dataset? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset saved as feature_engineered_dataset.xlsx and feature_engineered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    ")\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from scipy.stats import zscore, skew\n",
    "from numpy import log1p\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import cv2\n",
    "from skimage import color, feature\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_dataset():\n",
    "    try:\n",
    "        dataset_path = input(\"Enter the path of your dataset: \")\n",
    "        if dataset_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(dataset_path)\n",
    "            print(df.head())\n",
    "        else:\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def handle_missing_data(df):\n",
    "    while True:\n",
    "        try:\n",
    "            numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "            non_numeric_columns = df.select_dtypes(exclude=['number']).columns\n",
    "            strategy = input(\"Choose an imputation strategy for missing data (1. Mean for numeric, 2. Most frequent for non-numeric): \")\n",
    "\n",
    "            if strategy == \"1\":\n",
    "                imputer_numeric = SimpleImputer(strategy=\"mean\")\n",
    "                df[numeric_columns] = imputer_numeric.fit_transform(df[numeric_columns])\n",
    "            elif strategy == \"2\":\n",
    "                imputer_non_numeric = SimpleImputer(strategy=\"most_frequent\")\n",
    "                df[non_numeric_columns] = imputer_non_numeric.fit_transform(df[non_numeric_columns])\n",
    "\n",
    "            if df.isna().any().any():\n",
    "                print(\"Warning: NaN values found in the dataset. Please handle them.\")\n",
    "                choice = input(\"Do you want to remove rows with missing values? (y/n): \").lower()\n",
    "                if choice == \"y\":\n",
    "                    df = df.dropna()\n",
    "                else:\n",
    "                    print(\"Please choose a valid imputation strategy.\")\n",
    "                continue\n",
    "\n",
    "            return df\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"An error occurred due to missing values.\")\n",
    "            choice = input(\"Do you want to remove rows with missing values? (y/n): \").lower()\n",
    "            if choice == \"y\":\n",
    "                df = df.dropna()\n",
    "            else:\n",
    "                print(\"Please choose a valid imputation strategy.\")\n",
    "                continue\n",
    "\n",
    "def handle_categorical_data(df):\n",
    "    choice = input(\"Choose a method for handling categorical data (1. Label Encoding, 2. One-Hot Encoding): \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        for column in df.select_dtypes(include=[\"object\"]).columns:\n",
    "            df[column] = label_encoder.fit_transform(df[column])\n",
    "    elif choice == \"2\":\n",
    "        df = pd.get_dummies(df, columns=df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scaling_and_normalization(df):\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    choice = input(\"Choose a method for scaling and normalization (1. Standard Scaling, 2. Min-Max Scaling, 3. Robust Scaling): \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    elif choice == \"2\":\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    elif choice == \"3\":\n",
    "        scaler = RobustScaler()\n",
    "        df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_outliers(df):\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df_numeric = df[numeric_columns]\n",
    "    z_scores = zscore(df_numeric)\n",
    "    df_outliers = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "    return df_outliers\n",
    "\n",
    "def handle_skewed_distributions(df):\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df_numeric = df[numeric_columns]\n",
    "    skewed_features = df_numeric.apply(lambda x: skew(x))\n",
    "    skewed_features = skewed_features[abs(skewed_features) > 0.5]\n",
    "\n",
    "    for feature in skewed_features.index:\n",
    "        df[feature] = log1p(df[feature])\n",
    "\n",
    "    return df\n",
    "\n",
    "def exclude_features(df):\n",
    "    print(\"All features in the dataset:\")\n",
    "    print(df.columns)\n",
    "    exclude_columns = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a column to be excluded or 'n' to stop: \")\n",
    "        if user_input.lower() == 'n':\n",
    "            break\n",
    "        else:\n",
    "            exclude_columns.append(user_input)\n",
    "    df = df.drop(columns=exclude_columns, errors='ignore')\n",
    "    print(\"\\nDataset after excluding features:\")\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_feature(df):\n",
    "    print(\"Please implement your logic for adding new fields to the dataset in the add_feature() function.\")\n",
    "    print(\"Need Termination of the program.\")\n",
    "    time.sleep(3600)\n",
    "    try:\n",
    "        raise SystemExit\n",
    "    except SystemExit:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        sys.exit()\n",
    "    except SystemExit:\n",
    "        pass\n",
    "\n",
    "def handle_time_series_features(df):\n",
    "    date_columns = []\n",
    "    print(\"Potential columns for date and time conversion:\")\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            print(f\"  - {col} (Already in datetime format)\")\n",
    "        else:\n",
    "            print(f\"  - {col}\")\n",
    "    while True:\n",
    "        user_input = input(\"Enter the name of a column to convert to date and time (or 'done' to finish): \").lower()\n",
    "        if user_input == 'done':\n",
    "            break\n",
    "        elif user_input in df.columns:\n",
    "            date_columns.append(user_input)\n",
    "        else:\n",
    "            print(f\"Column '{user_input}' not found in the dataset. Please enter a valid column name.\")\n",
    "\n",
    "    if not date_columns:\n",
    "        print(\"No columns selected for date and time conversion.\")\n",
    "        return df\n",
    "    date_format_input = input(\"Enter the date and time format (or 'auto' to use detected format): \").strip()\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            if date_format_input.lower() == 'auto':\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = pd.to_datetime(df[col], format=date_format_input, errors='raise')\n",
    "            print(f\"Column '{col}' converted to datetime.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column '{col}' to datetime: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_nlp_features(df):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "    # Ask the user if they want to concatenate any columns\n",
    "    concatenate_columns = input(\"Do you want to concatenate any columns for NLP processing? (y/n): \").lower()\n",
    "    if concatenate_columns == 'y':\n",
    "        # Display available columns to the user\n",
    "        print(\"Available columns for concatenation:\")\n",
    "        print(df.columns)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                columns_to_concat = input(\"Enter the names of columns to concatenate (comma-separated): \").split(',')\n",
    "                new_column_name = input(\"Enter the name for the new concatenated column: \")\n",
    "                if all(col.strip() in df.columns for col in columns_to_concat):\n",
    "                    df[new_column_name] = df[columns_to_concat].astype(str).agg(' '.join, axis=1)\n",
    "                    print(\"\\nDataFrame after concatenation:\")\n",
    "                    print(df.head())\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid column name(s). Please enter valid column name(s).\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    text_column = input(\"Enter the name of the column containing NLP text data (you can use your concatenated column also): \")\n",
    "    if text_column not in df.columns:\n",
    "        print(f\"Error: The specified text column '{text_column}' does not exist in the dataset.\")\n",
    "        return df\n",
    "\n",
    "    # Tokenization\n",
    "    df['tokens'] = df[text_column].apply(lambda x: re.findall(r'\\b\\w+\\b', x.lower()))\n",
    "\n",
    "    # Display tokenized text\n",
    "    print(\"\\nTokenized Text:\")\n",
    "    print(df[['tokens']].head())\n",
    "\n",
    "    # Remove Stop Words\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    # Display text after stop words removal\n",
    "    print(\"\\nText after stop words removal:\")\n",
    "    print(df[['tokens']].head())\n",
    "\n",
    "    # Add tokenized text to the dataset\n",
    "    df['stopwords_removed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # Display DataFrame with tokenized text\n",
    "    print(\"\\nDataFrame with Tokenized Text:\")\n",
    "    print(df.head())\n",
    "    if not df[text_column].empty:\n",
    "        # TF-IDF Vectorization\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        if df[text_column].apply(lambda x: len(re.findall(r'\\b\\w+\\b', x.lower()))).sum() > 0:\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "            print(\"\\nTF-IDF DataFrame:\")\n",
    "            print(tfidf_df.head())\n",
    "\n",
    "            print(\"\\nNLP features engineered:\")\n",
    "            print(df.head())\n",
    "        else:\n",
    "            print(\"Error: Text data contains only stop words. TF-IDF vectorization cannot be performed.\")\n",
    "    else:\n",
    "        print(\"Error: Text data is empty. TF-IDF vectorization cannot be performed.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    try:\n",
    "        dataset_path = input(\"Enter the path of your dataset: \")\n",
    "        if dataset_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(dataset_path)\n",
    "            print(df.head())\n",
    "        else:\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def handle_vision_features(df):\n",
    "    print(\"Handling vision features...converting numerical, resizing, converting grey, normalizing, applying PCA and saving\")\n",
    "\n",
    "    def preprocess_image(image_path):\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            normalized_img = gray_img / 255.0\n",
    "            return normalized_img\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {str(e)}\")\n",
    "            return None\n",
    "    process_images = input(\"Do you want to process images and add them to the dataset? (y/n): \").lower()\n",
    "    if process_images == 'y':\n",
    "        image_path = input(\"Enter the path of the image file: \")\n",
    "        processed_image = preprocess_image(image_path.strip())\n",
    "        if processed_image is not None:\n",
    "            # Assuming you have a function apply_pca for PCA, you can apply it here\n",
    "            # pca_features = apply_pca(processed_images)\n",
    "            df['processed_images'] = [processed_image] * len(df)\n",
    "            print(\"Image processing and feature engineering completed.\")\n",
    "        else:\n",
    "            print(\"No valid image processing performed.\")\n",
    "    else:\n",
    "        print(\"No image processing performed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_highly_correlated_features(df):\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    correlation_matrix = df[numeric_columns].corr()\n",
    "    high_correlation_threshold = 0.8  \n",
    "    correlated_features = set()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > high_correlation_threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "\n",
    "    df = df.drop(columns=correlated_features, errors='ignore')\n",
    "    print(f\"Highly correlated features removed: {correlated_features}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_dimensionality_reduction(df):\n",
    "    while True:\n",
    "        reduction_method = input(\"Choose dimensionality reduction method (1. PCA, 2. SVD): \").lower()\n",
    "        try:\n",
    "            if reduction_method == \"1\":\n",
    "                n_components = int(input(\"Enter the number of components for PCA: \"))\n",
    "                if n_components > min(df.shape[0], df.shape[1]):\n",
    "                    raise ValueError(f\"Error: n_components={n_components} must be between 0 and min(n_samples, n_features)={min(df.shape[0], df.shape[1])}.\")\n",
    "                pca = PCA(n_components=n_components)\n",
    "                transformed_data = pca.fit_transform(df)\n",
    "                print(f\"PCA applied with {n_components} components.\")\n",
    "                break  # Exit the loop if input is valid\n",
    "            elif reduction_method == \"2\":\n",
    "                n_components = int(input(\"Enter the number of components for Truncated SVD: \"))\n",
    "                if n_components > min(df.shape[0], df.shape[1]):\n",
    "                    raise ValueError(f\"Error: n_components={n_components} must be between 0 and min(n_samples, n_features)={min(df.shape[0], df.shape[1])}.\")\n",
    "                svd = TruncatedSVD(n_components=n_components)\n",
    "                transformed_data = svd.fit_transform(df)\n",
    "                print(f\"Truncated SVD applied with {n_components} components.\")\n",
    "                break  \n",
    "            else:\n",
    "                print(\"Invalid dimensionality reduction method. Please choose either '1' for PCA or '2' for SVD.\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error: {ve}\")\n",
    "            retry = input(\"Do you want to retry entering the number of components? (y/n): \").lower()\n",
    "            if retry != \"y\":\n",
    "                break  \n",
    "    df_transformed = pd.DataFrame(transformed_data, columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "    return df_transformed\n",
    "\n",
    "def main():\n",
    "    df = load_dataset()\n",
    "\n",
    "    if df is not None:\n",
    "        dataset_types = [\"NLP\", \"Vision\", \"Time Series\", \"Other\"]\n",
    "        print(\"Your dataset types:\")\n",
    "        for i, dtype in enumerate(dataset_types, 1):\n",
    "            print(f\"{i}. {dtype}\")\n",
    "\n",
    "        while True:\n",
    "            dataset_type_input = input(\"Enter the number corresponding to the dataset type: \")\n",
    "            if dataset_type_input.isdigit():\n",
    "                dataset_type_index = int(dataset_type_input)\n",
    "                if 1 <= dataset_type_index <= len(dataset_types):\n",
    "                    dataset_type = dataset_types[dataset_type_index - 1].capitalize()\n",
    "                    break\n",
    "            print(\"Invalid input. Please enter a valid number.\")\n",
    "\n",
    "        exclude_option = input(\"Do you want to exclude any features? (y/n): \").lower()\n",
    "        if exclude_option == \"y\":\n",
    "            df = exclude_features(df)\n",
    "\n",
    "        add_feature_option = input(\"Do you want to add new fields to the dataset? (y/n): \").lower()\n",
    "        if add_feature_option == \"y\":\n",
    "            add_feature(df)\n",
    "\n",
    "        apply_imputation = input(\"Do you want to handle missing data? (y/n): \").lower()\n",
    "        if apply_imputation == \"y\":\n",
    "            df = handle_missing_data(df)\n",
    "\n",
    "        apply_categorical_handling = input(\"Do you want to handle categorical data? (y/n): \").lower()\n",
    "        if apply_categorical_handling == \"y\":\n",
    "            df = handle_categorical_data(df)\n",
    "\n",
    "        apply_scaling = input(\"Do you want to scale and normalize the data? (y/n): \").lower()\n",
    "        if apply_scaling == \"y\":\n",
    "            df = scaling_and_normalization(df)\n",
    "\n",
    "        apply_outlier_handling = input(\"Do you want to handle outliers? (y/n): \").lower()\n",
    "        if apply_outlier_handling == \"y\":\n",
    "            df = handle_outliers(df)\n",
    "\n",
    "        apply_skew_handling = input(\"Do you want to handle skewed distributions? (y/n): \").lower()\n",
    "        if apply_skew_handling == \"y\":\n",
    "            df = handle_skewed_distributions(df)\n",
    "\n",
    "        if dataset_type_input == \"3\":\n",
    "            handle_time_series_features(df)\n",
    "        elif dataset_type_input == \"1\":\n",
    "            handle_nlp_features(df)\n",
    "        elif dataset_type_input == \"2\":\n",
    "            handle_vision_features(df)\n",
    "\n",
    "        \n",
    "        apply_highly_correlated_features = input(\"Do you want to remove highly correlated features? (y/n): \").lower()\n",
    "        if apply_highly_correlated_features == \"y\":\n",
    "            df = handle_highly_correlated_features(df)\n",
    "\n",
    "        apply_dimensionality_reduction = input(\"Do you want to apply dimensionality reduction? (y/n): \").lower()\n",
    "        if apply_dimensionality_reduction == \"y\":\n",
    "            df = handle_dimensionality_reduction(df)\n",
    "\n",
    "        \n",
    "        print(\"Feature-engineered dataset:\")\n",
    "        print(df.head())\n",
    "\n",
    "        save_option = input(\"Do you want to save the feature-engineered dataset? (y/n): \").lower()\n",
    "        if save_option == \"y\":\n",
    "            output_path = \"feature_engineered_dataset\"\n",
    "            df.to_excel(f\"{output_path}.xlsx\", index=False)\n",
    "            df.to_csv(f\"{output_path}.csv\", index=False)\n",
    "            print(f\"Feature-engineered dataset saved as {output_path}.xlsx and {output_path}.csv\")\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9249be0-44e5-4b7b-a47a-c5efca3aff01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of your dataset:  book1.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  Value 1  Value 2  Value 3  Value 4       time\n",
      "0  Feature 1 is         1        2        3        4      21345\n",
      "1  Feature 2 am         2        3        4        5  324234243\n",
      "2      Feature 3        3        4        5        6  324324243\n",
      "3      Feature 4        4        5        6        7  234243243\n",
      "Your dataset types:\n",
      "1. NLP\n",
      "2. Vision\n",
      "3. Time Series\n",
      "4. Other\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the dataset type:  2\n",
      "Do you want to exclude any features? (y/n):  n\n",
      "Do you want to add new fields to the dataset? (y/n):  n\n",
      "Do you want to handle missing data? (y/n):  n\n",
      "Do you want to handle categorical data? (y/n):  n\n",
      "Do you want to scale and normalize the data? (y/n):  n\n",
      "Do you want to handle outliers? (y/n):  n\n",
      "Do you want to handle skewed distributions? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling vision features...converting numerical, resizing, converting grey, normalizing, applying PCA and saving\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to process images and add them to the dataset? (y/n):  y\n",
      "Enter the path of the image file:  Capture.PNG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing and feature engineering completed.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to remove highly correlated features? (y/n):  n\n",
      "Do you want to apply dimensionality reduction? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset:\n",
      "         Feature  Value 1  Value 2  Value 3  Value 4       time   \n",
      "0  Feature 1 is         1        2        3        4      21345  \\\n",
      "1  Feature 2 am         2        3        4        5  324234243   \n",
      "2      Feature 3        3        4        5        6  324324243   \n",
      "3      Feature 4        4        5        6        7  234243243   \n",
      "\n",
      "                                    processed_images  \n",
      "0  [[0.9411764705882353, 0.9411764705882353, 0.94...  \n",
      "1  [[0.9411764705882353, 0.9411764705882353, 0.94...  \n",
      "2  [[0.9411764705882353, 0.9411764705882353, 0.94...  \n",
      "3  [[0.9411764705882353, 0.9411764705882353, 0.94...  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the feature-engineered dataset? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset saved as feature_engineered_dataset.xlsx and feature_engineered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# for Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "042da8f1-1fdc-4556-a5c6-a9e1ffd96315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of your dataset:  book1.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  Value 1  Value 2  Value 3  Value 4       time\n",
      "0  Feature 1 is         1        2        3        4      21345\n",
      "1  Feature 2 am         2        3        4        5  324234243\n",
      "2      Feature 3        3        4        5        6  324324243\n",
      "3      Feature 4        4        5        6        7  234243243\n",
      "Your dataset types:\n",
      "1. NLP\n",
      "2. Vision\n",
      "3. Time Series\n",
      "4. Other\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the dataset type:  3\n",
      "Do you want to exclude any features? (y/n):  n\n",
      "Do you want to add new fields to the dataset? (y/n):  n\n",
      "Do you want to handle missing data? (y/n):  n\n",
      "Do you want to handle categorical data? (y/n):  n\n",
      "Do you want to scale and normalize the data? (y/n):  n\n",
      "Do you want to handle outliers? (y/n):  n\n",
      "Do you want to handle skewed distributions? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential columns for date and time conversion:\n",
      "  - Feature\n",
      "  - Value 1\n",
      "  - Value 2\n",
      "  - Value 3\n",
      "  - Value 4\n",
      "  - time\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of a column to convert to date and time (or 'done' to finish):  time\n",
      "Enter the name of a column to convert to date and time (or 'done' to finish):  done\n",
      "Enter the date and time format (or 'auto' to use detected format):  auto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'time' converted to datetime.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to remove highly correlated features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features removed: {'Value 4', 'Value 2', 'Value 3'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to apply dimensionality reduction? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset:\n",
      "         Feature  Value 1                          time\n",
      "0  Feature 1 is         1 1970-01-01 00:00:00.000021345\n",
      "1  Feature 2 am         2 1970-01-01 00:00:00.324234243\n",
      "2      Feature 3        3 1970-01-01 00:00:00.324324243\n",
      "3      Feature 4        4 1970-01-01 00:00:00.234243243\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the feature-engineered dataset? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset saved as feature_engineered_dataset.xlsx and feature_engineered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "#Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fe59ca4-65ab-45a2-9d89-19c5ac302627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of your dataset:  book1.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature  Value 1  Value 2  Value 3  Value 4       time\n",
      "0  Feature 1 is         1        2        3        4      21345\n",
      "1  Feature 2 am         2        3        4        5  324234243\n",
      "2      Feature 3        3        4        5        6  324324243\n",
      "3      Feature 4        4        5        6        7  234243243\n",
      "Your dataset types:\n",
      "1. NLP\n",
      "2. Vision\n",
      "3. Time Series\n",
      "4. Other\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the dataset type:  4\n",
      "Do you want to exclude any features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features in the dataset:\n",
      "Index(['Feature', 'Value 1', 'Value 2', 'Value 3', 'Value 4', 'time'], dtype='object')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a column to be excluded or 'n' to stop:  Value 3\n",
      "Enter a column to be excluded or 'n' to stop:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after excluding features:\n",
      "         Feature  Value 1  Value 2  Value 4       time\n",
      "0  Feature 1 is         1        2        4      21345\n",
      "1  Feature 2 am         2        3        5  324234243\n",
      "2      Feature 3        3        4        6  324324243\n",
      "3      Feature 4        4        5        7  234243243\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to add new fields to the dataset? (y/n):  n\n",
      "Do you want to handle missing data? (y/n):  y\n",
      "Choose an imputation strategy for missing data (1. Mean for numeric, 2. Most frequent for non-numeric):  1\n",
      "Do you want to handle categorical data? (y/n):  y\n",
      "Choose a method for handling categorical data (1. Label Encoding, 2. One-Hot Encoding):  1\n",
      "Do you want to scale and normalize the data? (y/n):  y\n",
      "Choose a method for scaling and normalization (1. Standard Scaling, 2. Min-Max Scaling, 3. Robust Scaling):  1\n",
      "Do you want to handle outliers? (y/n):  y\n",
      "Do you want to handle skewed distributions? (y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to remove highly correlated features? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features removed: {'Value 4', 'Value 2', 'time', 'Value 1'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to apply dimensionality reduction? (y/n):  y\n",
      "Choose dimensionality reduction method (1. PCA, 2. SVD):  2\n",
      "Enter the number of components for Truncated SVD:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error: n_components=2 must be between 0 and min(n_samples, n_features)=1.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to retry entering the number of components? (y/n):  y\n",
      "Choose dimensionality reduction method (1. PCA, 2. SVD):  1\n",
      "Enter the number of components for PCA:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA applied with 1 components.\n",
      "Feature-engineered dataset:\n",
      "        PC1\n",
      "0 -1.341641\n",
      "1 -0.447214\n",
      "2  0.447214\n",
      "3  1.341641\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the feature-engineered dataset? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered dataset saved as feature_engineered_dataset.xlsx and feature_engineered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc63bf-aa88-4eca-858c-fff61cdcee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
